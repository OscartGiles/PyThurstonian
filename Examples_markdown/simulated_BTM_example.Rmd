---
title: "BTM"
author: "Oscar T Giles"
date: "9 October 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe', echo = FALSE}
import sys
print(sys.version)
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/ProgramData/Anaconda3//Library/plugins/platforms'
```

## Using PyThurstonian to fit a Bayesian Thurstonian Model (BTM)

Here we provide a short example of fitting a BTM using a simulated dataset.

First we import PyThurstonian along with a few other useful packages.

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
rootpath = 'M:\Transport_Studies\Work_Projects\PSI\Publication_Projects\PyThurstonian'
sys.path.append(rootpath)
from PyThurstonian import thurstonian, simulate_data, run_sample, hdi
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
from multiprocessing import Process #If we want to use multiprocessing
```


## Create a simulated data set

Now we can create a simulated dataset using the simulate_data function from PyThurstonian. 

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
##Dataset properties
K = 3 # Number of items being ranked
J = 10 # Number of subjects
L = 1 #Number of trials (Subjects can contribute more than one trial to the same condition)
C = 2 #Number of conditions (Number of conditions. Here we assume a within subjects design, but PyThurstonian will automatically handle different design types)

#Beta parameters
beta = np.array([[0.0, 1.0, 2.0],
                [0.0, 0.1, 0.2]])        

data, sim_scale = simulate_data(K, J, L, C, beta = beta, seed = 4354356) #Set a seed to recreate the same dataset
```

Lets data a look at the dataset, which is held in a Pandas dataframe in long (tidy) format.

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
print(data)
```

We can see that there are three columns which identify the subject, condition and trial number (we can have multiple trials per participant). The remaining columns provide the ranking for each item and must be labeled Y1, Y2,...,YK. Here we only have one factor called condition, but we could add more columns to identify any number of factors. 

## Creating a Thurstonian model instance

Next we create specify our BTM. Before we start we need to convert the subject column of our dataframe to a numeric factor and ensure that it starts from 1, as PyThurstonian makes use of Stan, which indexes from 1 (pandas automatically starts the factor from 0).

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
data['Subj'] = pd.factorize(data['Subj'])[0]+1
```

Now we are ready to specify our model, which we achieve by creating an instance of the thurstonian class:

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
myThurst = thurstonian(design_formula = '~0+Condition', data = data, subject_name = "Subj")  
```

The first argument to the thurstonian class is a design formula. This uses a simple lme4 style syntax, and uses Patsy (https://patsy.readthedocs.io) to create a design matrix under the hood. The second and third arguments pass the dataframe and specify the name of the column containing the subject factor. 


## Sampling from the model
Next we sample from the model by calling the sample method on our thurstonian instance:

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe', eval = FALSE}
myThurst.sample( iter = 5000, adapt_delta = 0.99, rep_data = True)
```

However, this will only sample on a single core, and it can be beneficial to run multiple samples in parallel. In PyThurstonian this requires a couple of additional steps:

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe', eval = FALSE}
#Call this function to prepare everything to run on multiple cores
myThurst.pre_sample()        

#Sample with multiprocessing
P = [Process(target=run_sample, args = (5000,), kwargs = {'temp_fnum': i})  for i in range(cores)]     

#Start sampling and wait for everything to join
for p in P:
    p.start()            
for p in P:
    p.join()

#This must be called after sampling - PyThurstonian will then gather all the results
myThurst.post_sample()
```

Notice that we passed a function called run_sample to the Process function. This function is defined outside of the Thurstonian class to enable Pythons Multiprocessing library to be used. 


PyThurstonian will sample from the posterior defined by the user specified model. Finally we can save the results to file for later analysis:

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe', eval = FALSE}
myThurst.save_samples('MCMC/simple_samples') 
```



If we wish to reload these samples we simply call:

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
myThurst.load_samples('MCMC/simple_samples')
```


## Exploring the posterior

PyThurstonian comes with a number of convience functions for plotting the data and assessing the model fits. First we will plot our raw data, before examining the aggregate rankings, agreement between participants and performing contrasts between our two conditions.

### Raw data plots
The following will plot the proportion of subjects who selected each possible rank for both conditions C1 and C2. We use the thurstonian instance's plot_data method to do the plotting, which takes a dictionary of factor levels and a matplotlib axes as arguments:


```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
conds = ['C1', 'C2']    
fig, ax = plt.subplots(1,2, sharey = True, figsize = (3, 2))

#Iterate over every condition
for i, cond in enumerate(conds):              

    myThurst.plot_data(level_dict = {'Condition': cond}, ax = ax[i])
    ax[i].set_title("Condition: {}".format(cond))
    ax[i].set_ylim([0,1])
    
plt.suptitle("Data Summary")

plt.subplots_adjust(top=0.76,
                    bottom=0.34,
                    left=0.185,
                    right=0.952,
                    hspace=0.2,
                    wspace=0.327)

plt.show()
```
   
### Aggregate rankings

Next we can plot the Bayesian estimate of the aggregate ranking for each condition. 

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
fig, ax = plt.subplots(1,2, sharey = True, figsize = (3, 2))
    
#Iterate over every condition
for i, cond in enumerate(conds):                

  myThurst.plot_aggregate(level_dict = {'Condition':cond}, ax = ax[i])            
  ax[i].set_title("Condition: {}".format(cond))

plt.suptitle("Bayesian Aggregate")

plt.subplots_adjust(top=0.765,
                    bottom=0.34,
                    left=0.184,
                    right=0.952,
                    hspace=0.2,
                    wspace=0.322)
                    
plt.show()
```

### Agreement

Here we plot the kendall's W for both the data sets, and over the posterior predictive distribution. 

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
fig, ax = plt.subplots(1, 2, figsize = (4, 2), sharex = True, sharey = True)
myThurst.plot_kendall_W(level_dict = {'Condition':  'C1'}, ax = ax[0])
myThurst.plot_kendall_W(level_dict = {'Condition':  'C2'}, ax = ax[1])
plt.suptitle("Bayesian Agreement")
plt.show()
```

### Contrasts

Finally we can perform a Bayesian contrast between the two conditions, summarised using kendall's tau distance. 

```{python, engine.path = 'C:/ProgramData/Anaconda3/Python.exe'}
fig, ax = plt.subplots(1,1, sharey = True, figsize = (2,1.5))              
 
myThurst.plot_contrast(level_dict_1 = {'Condition':'C1'}, 
                       level_dict_2 = {'Condition':'C2'},
                       ax = ax)            
ax.set_ylim([0, 1])
ax.set_title("Contrast: C1 vs C2")
plt.tight_layout()
plt.show()
```